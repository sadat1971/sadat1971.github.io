<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>in</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2>Projects</h2>
<p>(Last Update 01-10-2024)</p>
<h3>LLM for Mental Health Support (Ongoing)</h3>
<p>– Traditional Large Language Model (LLMs), although skilled in
various tasks, do not possess the necessary specialization to offer
effective mental health support. In response to the critical need for
mental health support, particularly for individuals facing distressful
situations and heightened risks to their well-being, and posting to
various online support groups, the initiative aims to develop a
specialized LLM.</p>
<p>–The objective is to develop an LLM program specifically designed to
assist individuals facing mental health challenges. Designing a system
that can analyze the patient's personality, the level of risk, and
provide appropriate answers/comments is a significant challenge.</p>
<p>– Orchastreated a comprehensive step-by-step solution design.
Performed intelligent prompt engineering with several SOTA LLMs (Llama2,
GPT-3, Mistral-7b), supervised fine-tuning, and fine-tuning with
Reinforcement Learning with Human and AI feedback.</p>
<p>– While the initiative is still in progress, early results show
positive progress toward the creation of an LLM specifically designed
for mental health support. The coordinated use of prompt engineering and
fine-tuning approaches is resulting in a more complex understanding of
users' mental states, allowing the model to provide intelligent and
empathic replies.</p>
<p align="center">
  <img src="/docs/images/robot_support.png" width="30%">
</p>

<h3>Deceptive Domain Transfer</h3>
<p>– The web being a fertile ground for deceptive text in different
domains like Fake News, Phishing Emails, Fake Reviews, and Rumors, the
SOTA detection methods are domain-specific, and as such, detection model
in one domain doesnot work for the other. However, a major discrepancy
in the data distribution in these domains requires a method of deceptive
knowledge transfer.</p>
<p>– Conducted several in-depth ML-based analyses for a successful
domain transfer that includes intermediate-layer concatenation, holistic
training, and multi-task learning. Used cutting-edge techniques like
BERT, character-level CNN, Sentence-BERT, and attention-based LSTM.</p>
<p>– Built a sophisticated domain-transfer technique, which was the
first in the field, and improved performance over the self-domain
training strategy by up to 18% in F1-score. Published results in three
conferences: RANLP ’21, and ‘23, and SOCINFO’ 22.</p>
<p align="center">
  <img src="/docs/images/domtransfer.png" width="60%">
</p>

<h3>Phishing Emails and Psychological Traits</h3>
<p>– Despite numerous attempts of phishing email detection, attackers
are ever-evolving, often outsmarting the efforts. The project aimed to
improve phishing email detection by detecting and utilizing unique
psychological traits present in phishing emails.</p>
<p>– Identified and quantified three dominant psychological traits in
phishing emails (PPT): A Sense of Urgency, Inducing Fear by Threatening,
and Enticement with Desire, using BERT, Sentence-BERT,
Character-level-CNN. Also employed GPT-2 for balanced training.</p>
<p>– Achieved a significant performance improvement of 4.54% in F1-score
by using the PPT scores in phishing email detection over the SOTA
approaches, with Fear being the most prominent PPT. Published result in
WBC ‘22, and won the best paper award.</p>
<p align="center">
  <img src="/docs/images/PPT.png" width="60%">
</p>

<h3>Collusion Scam in YouTube Comments</h3>
<p>– The YouTube comment section has long been plagued by scammers,
despite efforts to curb their activities, and now, a new type of scam,
known as the "Collusion Scam," has emerged as a prominent threat within
YouTube comments, particularly in the context of the cryptocurrency
market.</p>
<p>– Performed data collection using YouTube API, and conducted analysis
and detection of collusion scams using tf-idf, logistic regression,
BERT, and zero-shot LLM training.</p>
<p>– Built the first dataset for collusion scam, developed deep learning
techniques for successful collusion scam detection, and demonstrated the
efficacy of leveraging metadata like the timespan, like count, and age
of the channel in the detection process. Published one conference paper
in RANLP ‘23.</p>
<p align="center">
  <img src="/docs/images/colscam.png" width="70%">
</p>

<h3>Disagreements in Hate Speech Annotation</h3>
<p>– Preserving the difference of opinion while labeling derogatory
textual content can help us capture many important social phenomena like
diversity of viewpoints. In this project, we aim to model subjectivity
in the annotation of hate speech.</p>
<p>– Leveraged four datasets from SemEval-2023 Task 11 to explore and
handle disagreement in the annotation process effectively. Performed
training by fine-tuning a BERT model and compared a post-aggregation and
disagreement-targeted learning approach.</p>
<p>– Demonstrated the effectiveness of individual annotator modeling and
the importance of metadata association. Achieved a top ten spot in terms
of cross-entropy score, and published in SemEval ‘23 conference.</p>
<p align="center">
  <img src="/docs/images/hate.png" width="60%">
</p>

<h3>Emotion Forecasting in Dyadic Conversation</h3>
<p>– Emotion forecasting is a novel task, aiming to predict a speaker's
future emotional state based on their past and current audiovisual cues.
Unlike traditional emotion recognition, it involves predicting emotions
in advance and demands innovative problem formulations and modeling
techniques.</p>
<p>– Explored two distinct forecasting windows: utterance forecasting
and time forecasting. Assessed the impact of incorporating both past and
current audiovisual cues in the emotion forecasting process. Compared
three deep networks: FCDNN, D-BLSTM, D-LSTM.</p>
<p>– Demonstrated the superiority of dynamic models in the forecasting
process by an improvement of 3% on average. Published the work in the
prestigious FG ‘19 conference.</p>
<p align="center">
  <img src="/docs/images/emo.png" width="60%">
</p>
</body>
</html>
